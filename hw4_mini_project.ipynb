{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nancii-jif/305b_final/blob/main/hw4_mini_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ-AwbZ4ySCJ"
      },
      "source": [
        "## 0.0 Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_5IXGh6OOBZV"
      },
      "outputs": [],
      "source": [
        "# torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "torch.manual_seed(305)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsgHl9JCuGBS"
      },
      "source": [
        "We set default values for some global hyperparameters, but feel free to change these during development as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A_Z5Jh74DH_E"
      },
      "outputs": [],
      "source": [
        "# Global hyperparameters\n",
        "SMALL_ITERS = 1000\n",
        "LARGE_ITERS = 2000\n",
        "EVAL_ITERS = 100\n",
        "CONTEXT_WINDOW_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53dGz7ExDkUv",
        "outputId": "1d25f7cf-bbfc-4e5b-9fa2-87fb979f168b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n"
          ]
        }
      ],
      "source": [
        "# download the tiny shakespeare dataset\n",
        "input_file_path = 'input.txt'\n",
        "\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1 Helper Functions"
      ],
      "metadata": {
        "id": "xoKDoV4liTS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for getting batches of data\n",
        "def get_batch(split, context_window_size, device, batch_size=32, token_type='morf'):\n",
        "    \"\"\"\n",
        "    generate a small batch of data of inputs x and targets y\n",
        "\n",
        "    Args:\n",
        "        split: 'train' or 'val'\n",
        "        device: 'cpu' or 'cuda' (should be 'cuda' if available)\n",
        "    \"\"\"\n",
        "    if token_type == 'bpe':\n",
        "        data = train_data_bpe if split == 'train' else val_data_bpe\n",
        "    elif token_type == 'morf':\n",
        "        data = train_data_morf if split == 'train' else val_data_morf\n",
        "    else:\n",
        "      data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - context_window_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+context_window_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+context_window_size+1] for i in ix])\n",
        "    # x_labels = torch.stack([train_labels[i:i+context_window_size] for i in ix])\n",
        "    # y_labels = torch.stack([train_labels[i+1:i+context_window_size+1] for i in ix])\n",
        "    # x = torch.cat((x, x_labels), dim=2)\n",
        "    # y = torch.cat((y, y_labels), dim=2)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    # x_labels = x_labels.to(device)\n",
        "    # y_labels = y_labels.to(device)\n",
        "    return x, y\n",
        "\n",
        "# helper function for tracking loss during training\n",
        "# given to you\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, eval_iters, context_window_size, device, token_type='morf'):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      model: model being evaluated\n",
        "      eval_iters: number of batches to average over\n",
        "      context_window_size: size of the context window\n",
        "      device: 'cpu' or 'cuda' (should be 'cuda' if available)\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, context_window_size, device, token_type=token_type)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    return out"
      ],
      "metadata": {
        "id": "-jQIv62fe9F5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dataLoader:\n",
        "  def __init__(self, data, token_type='bpe', tokenizer_model=None):\n",
        "    self.data = data\n",
        "    self.tokenizer_model = tokenizer_model\n",
        "    self.n = len(data)\n",
        "    self.vocab = sorted(set(self.data))\n",
        "    if not tokenizer_model:\n",
        "      self.vocab_size = len(self.vocab)\n",
        "    else:\n",
        "      self.vocab_size = tokenizer_model.vocab_size()\n",
        "    self.stoi = { seg:i for i,seg in enumerate(self.vocab) }\n",
        "    self.itos = { i:seg for i,seg in enumerate(self.vocab) }\n",
        "    self.token_type = token_type\n",
        "    self.tokenizer_model = tokenizer_model\n",
        "    self.train_data = None\n",
        "    self.val_data = None\n",
        "\n",
        "  def encoder(self, tokens):\n",
        "    return [self.stoi[t] for t in tokens]\n",
        "\n",
        "  def decoder(self, indices):\n",
        "    return ''.join([self.itos[i] for i in indices])\n",
        "\n",
        "  def get_train_val(self, train_test_split=.9):\n",
        "    train_data_raw = self.data[:int(self.n*0.9)]\n",
        "    val_data_raw = self.data[int(self.n*0.9):]\n",
        "    if self.tokenizer_model is not None:\n",
        "      self.train_data = torch.tensor(self.tokenizer_model.encode(train_data_raw, out_type=int))\n",
        "      self.val_data = torch.tensor(self.tokenizer_model.encode(val_data_raw, out_type=int))\n",
        "    else:\n",
        "      self.train_data = torch.tensor(self.encoder(train_data_raw, out_type=int))\n",
        "      self.val_data = torch.tensor(self.encoder(val_data_raw, out_type=int))\n",
        "    return self.train_data, self.val_data\n",
        "\n"
      ],
      "metadata": {
        "id": "T2SCnE6WZcRI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2 Evaluations"
      ],
      "metadata": {
        "id": "5zYr71M2WO8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google-research/bleurt.git\n",
        "%cd bleurt\n",
        "!pip install -r requirements.txt\n",
        "!pip install ."
      ],
      "metadata": {
        "id": "LtPpnj5bqn49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip\n",
        "!unzip BLEURT-20.zip\n",
        "\n",
        "from bleurt import score\n",
        "bleurt_scorer = score.BleurtScorer(\"BLEURT-20\")"
      ],
      "metadata": {
        "id": "oqG-Lm9rqp4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def eval_context_generation(contexts, targets, tlm=None, tokenizer_model=None, token_type='morf', scorer=None):\n",
        "  cond_gens = []\n",
        "  bleurt_scores = []\n",
        "  for i, context in enumerate(contexts):\n",
        "    if token_type == 'bpe' and tokenizer_model is not None:\n",
        "      context_tokens = torch.tensor(tokenizer_model.encode(context, out_type=int), device=device).reshape(1, -1)\n",
        "      cond_gen = (tlm.generate(context_tokens, max_new_tokens=CONTEXT_WINDOW_SIZE)[0].tolist())\n",
        "      cond_gens.append(tokenizer_model.decode(cond_gen, out_type='str'))\n",
        "\n",
        "\n",
        "    elif token_type == 'morf' and tokenizer_model is not None:\n",
        "      context_cleaned = TOKEN_RE.findall(context)\n",
        "      context_tokens = []\n",
        "      for word in context_cleaned:\n",
        "        if word.isalpha():\n",
        "          segs, _ = tokenizer_model.viterbi_segment(word)\n",
        "          context_tokens.extend(s for s in segs)\n",
        "        else:\n",
        "          context_tokens.append(word) # non-letter char\n",
        "      context_tokens = torch.tensor(dataloader_morf.encode(context_tokens), device=device).reshape(1, -1)\n",
        "      cond_gen = (tlm.generate(context_tokens, max_new_tokens=CONTEXT_WINDOW_SIZE)[0].tolist())\n",
        "      cond_gens.append(dataloader_morf.decode(cond_gen))\n",
        "    if scorer is not None:\n",
        "      bleurt_score = scorer.score(references=[targets[i]], candidates=[cond_gen])\n",
        "      bleurt_scores.append(bleurt_score)\n",
        "    return_df = pd.DataFrame({'context': contexts, 'target': targets, 'cond_gen': cond_gens})\n",
        "    return return_df, bleurt_scores"
      ],
      "metadata": {
        "id": "WwpT7L1kWOF1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bleurt import score\n",
        "bleurt_scorer = score.BleurtScorer(\"BLEURT-20\")\n",
        "ref = \"\"\"So Romeo would, were he not Romeo call'd,\n",
        "Retain that dear perfection which he owes\n",
        "Without that title. Romeo, doff thy name,\n",
        "And for that name which is no part of thee\n",
        "Take all myself.\"\"\"\n",
        "gen1 = \"\"\"Which is grust, be mind will what is arms:\n",
        "I am shall'd enter he's raised up fortune\n",
        "Depose upon that up the vows their isonace ailing,\n",
        "And trust by the death thrals up with his sshour\n",
        "Acquainful for Buckingham.\n",
        "\n",
        "MERCUTIO:\n",
        "Then chast that marry here!\n",
        "I co\n",
        "\"\"\"\n",
        "gen2 = \"\"\"Isum, i, and by to find better Tower.\n",
        "Well, a lord, that I know this man's king.\n",
        "Soft and here art thou split'st Romeo, man of my sad\n",
        "My bounty of state and Dick, he sought to signify\n",
        "And aid those that with no cause may contain out all\n",
        "For though not cause were they chosen, our\"\"\"\n",
        "bleurt_score = bleurt_scorer.score(references=[ref], candidates=[gen2])\n",
        "print(bleurt_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqAAYHiGm1PQ",
        "outputId": "e8001177-b322-4d64-da11-627e196b23e9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.26428818702697754]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_loss_and_perplexity(train_losses, val_losses, train_perps, val_perps, transformer_models=['vanilla', 'bpe', 'morf', 'bias']):\n",
        "    \"\"\"\n",
        "    Plots training/validation loss and perplexity for multiple models.\n",
        "\n",
        "    Args:\n",
        "        train_losses: list of lists, shape (num_models, num_iters)\n",
        "        val_losses: list of lists, same\n",
        "        train_perps: list of lists, same\n",
        "        val_perps: list of lists, same\n",
        "        transformer_models: list of model names, same order as data\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # --- Loss Plot ---\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for i, model in enumerate(transformer_models):\n",
        "        plt.plot(train_losses[i], label=f\"{model} - train\", linestyle='-')\n",
        "        plt.plot(val_losses[i], label=f\"{model} - val\", linestyle='--')\n",
        "    plt.title(\"train and val loss across models\")\n",
        "    plt.xlabel(\"iter\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # --- Perplexity Plot ---\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for i, model in enumerate(transformer_models):\n",
        "        plt.plot(train_perps[i], label=f\"{model} - train\", linestyle='-')\n",
        "        plt.plot(val_perps[i], label=f\"{model} - val\", linestyle='--')\n",
        "    plt.title(\"train and val perplexity across models\")\n",
        "    plt.xlabel(\"iter\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "JTvB94RLjGrF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Baselines"
      ],
      "metadata": {
        "id": "0c0NcqDESTJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.0 Vanilla Transformer (character-level tokenization)"
      ],
      "metadata": {
        "id": "CS2wPeJMeaRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")"
      ],
      "metadata": {
        "id": "cNPvBGsQV-Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGuUmypgOHVm"
      },
      "source": [
        "### 1.1 Byte Pair Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0j-l9Up4OHVm"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87QrPKIOOHVm",
        "outputId": "376be889-1b0a-4de8-c86b-b55fed3d9df0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.Train(input='input.txt',\n",
        "                               model_prefix='bpe_baseline',\n",
        "                               vocab_size=3000,\n",
        "                               model_type='bpe',\n",
        "                               character_coverage=1.0,\n",
        "                               add_dummy_prefix=False,\n",
        "                               user_defined_symbols = ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?'])\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('bpe_baseline.model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader_bpe = dataLoader(data, token_type='bpe', tokenizer_model=sp)\n",
        "train_data_bpe, val_data_bpe = data_loader_bpe.get_train_val()\n",
        "vocab_size_bpe = data_loader_bpe.vocab_size\n",
        "print(f\"train has {len(train_data_bpe):,} tokens\")\n",
        "print(f\"val has {len(val_data_bpe):,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpAGU2eFbwr-",
        "outputId": "87f92bdd-a78f-40a2-e953-5f9b4869f417"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 325,421 tokens\n",
            "val has 37,583 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DQCngBHOHVm",
        "outputId": "ef53cc94-70c6-4c31-e396-c103784be68e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 325,421 tokens\n",
            "val has 37,583 tokens\n"
          ]
        }
      ],
      "source": [
        "bpe_vocab_size = sp.vocab_size()\n",
        "n = len(data)\n",
        "train_chars = data[:int(n*.9)]\n",
        "val_chars = data[int(n*.9):]\n",
        "train_data_bpe = torch.tensor(sp.encode(train_chars, out_type=int))\n",
        "val_data_bpe = torch.tensor(sp.encode(val_chars, out_type=int))\n",
        "print(f\"train has {len(train_data_bpe):,} tokens\")\n",
        "print(f\"val has {len(val_data_bpe):,} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BPE Transformer (=Vanilla)"
      ],
      "metadata": {
        "id": "PSYXRGOVcber"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O_eBPiT-Yy0q"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, context_window_size, embed_size=384):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          head_size: int, size of the head embedding dimension (K)\n",
        "          context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "          embed_size: int, size of the token embedding dimension (D)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        self.key = nn.Linear(embed_size, head_size, bias=False) # query\n",
        "        self.query = nn.Linear(embed_size, head_size, bias=False) # key\n",
        "        self.value = nn.Linear(embed_size, embed_size, bias=False)\n",
        "\n",
        "        # not a param of the model, so registered as a buffer\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(context_window_size, context_window_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x: (B,T,D) tensor of token embeddings\n",
        "\n",
        "        Returns:\n",
        "          (B,T,D) tensor of attention-weighted token embeddings\n",
        "        \"\"\"\n",
        "        _, T, _ = x.size()\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "        attn_scores = q @ k.transpose(-2, -1)\n",
        "        masked_scores = attn_scores.masked_fill(self.tril == 0, float('-inf'))\n",
        "        masked_scores = masked_scores / self.head_size ** .5\n",
        "\n",
        "        attn = F.softmax(masked_scores, dim=-1) @ v\n",
        "        return attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6vb8NU_s6Vfg"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, context_window_size, num_heads, head_size, embed_size=384):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "            num_heads: int, number of heads (H)\n",
        "            head_size: int, size of the head embedding dimension\n",
        "            embed_size: int, size of the token embedding dimension\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO, your code below\n",
        "        self.head_size = head_size\n",
        "        self.heads = nn.ModuleList([Head(head_size, context_window_size, embed_size) for _ in range(num_heads)])\n",
        "        self.linear = nn.Linear(num_heads * embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_list = [head.forward(x) for head in self.heads]\n",
        "        mhsa = torch.cat(attn_list, dim=-1)\n",
        "        mhsa = self.linear(mhsa)\n",
        "        return mhsa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1GbGqwKWJzOK"
      },
      "outputs": [],
      "source": [
        "# run this cell to initialize this deep learning module that you should use in the code your write later\n",
        "# you don't need to edit this layer\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity\n",
        "        Given to you, you don't need to write any code here!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hUDbIv9eISkf"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\" Transformer block: communication across sequence length, followed by communication across embedding space\n",
        "        Uses multi-headed attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_size)\n",
        "        self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        # TODO: your code below\n",
        "        head_size = embed_size // num_heads\n",
        "        self.feed_forward = FeedForward(embed_size)\n",
        "        self.atten_heads = MultiHeadAttention(context_window_size, num_heads, head_size, embed_size)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.atten_heads(self.ln1(x)) # communication over sequence length\n",
        "        x = x + self.feed_forward(self.ln2(x)) # communication across embedding space\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "t2veTg9N3ufJ"
      },
      "outputs": [],
      "source": [
        "class TransformerLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6, n_layers=6):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              vocab_size: int, number of tokens in the vocabulary (V)\n",
        "              context_window_size: int, size of the context window (T)\n",
        "              embed_size: int, embedding size (D)\n",
        "              num_heads: int, number of heads (H)\n",
        "              n_layers: int, number of layers (M)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.context_window_size = context_window_size\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(vocab_size,\n",
        "                             context_window_size,\n",
        "                             embed_size=embed_size,\n",
        "                             num_heads=num_heads)\n",
        "            for _ in range(n_layers)])\n",
        "\n",
        "        # final layer norm\n",
        "        self.ln_f = nn.LayerNorm(embed_size)\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "        # good initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Agrgs:\n",
        "            token_ids: tensor of integers, provides the contet, shape (B, T)\n",
        "            targets: tensor of integers, provides the tokens we are preidcitng, shape (B, T)\n",
        "        \"\"\"\n",
        "        B, T = token_ids.shape\n",
        "\n",
        "        # token_ids and targets are both (B, T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(token_ids) # (B, T, D)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, D)\n",
        "        x = tok_emb + pos_emb # (B, T, D)\n",
        "\n",
        "        # TODO: your code below\n",
        "        blocked = self.blocks(x)\n",
        "        normalized = self.ln_f(blocked)\n",
        "        logits = self.lm_head(normalized)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits_flat = logits.view(-1, logits.size(-1))\n",
        "            targets_flat = targets.view(-1)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_ids: tensor of integers forming the context, shape (B, T)\n",
        "            max_new_tokens: int, max number of tokens to generate\n",
        "        \"\"\"\n",
        "        # TOOD, your code below\n",
        "        T = token_ids.size(-1)\n",
        "        curr_T = T\n",
        "        for _ in range(max_new_tokens):\n",
        "          token_ids_subset = token_ids[:, -self.context_window_size:]\n",
        "          logits, _ = self.forward(token_ids_subset)\n",
        "          logits = logits[:, -1, :] # for each entry in the batch, gets the last token\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          next_token = torch.multinomial(probs, num_samples=1) # sample next token\n",
        "          token_ids = torch.cat((token_ids, next_token), dim=1)\n",
        "        return token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsnbDpdhLeKo",
        "outputId": "ab83d8c3-12b7-4c7c-b420-3a4e468dcb47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0\n",
            "step 0: train loss 8.1313, val loss 8.1321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 200/2000 [00:49<05:47,  5.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 201/2000 [01:04<2:17:49,  4.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: train loss 4.9446, val loss 5.0268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 400/2000 [01:44<05:23,  4.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 401/2000 [01:58<1:54:48,  4.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: train loss 4.5320, val loss 4.7117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 600/2000 [02:37<04:35,  5.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 601/2000 [02:51<1:41:52,  4.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: train loss 4.1970, val loss 4.5186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 800/2000 [03:31<03:53,  5.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 801/2000 [03:45<1:26:19,  4.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: train loss 3.9392, val loss 4.4183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 1000/2000 [04:25<03:25,  4.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1001/2000 [04:39<1:12:23,  4.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: train loss 3.6689, val loss 4.3252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 1200/2000 [05:19<02:37,  5.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 1201/2000 [05:33<57:40,  4.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: train loss 3.4021, val loss 4.3377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 1400/2000 [06:12<01:57,  5.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 1401/2000 [06:26<43:15,  4.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: train loss 3.0929, val loss 4.3715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 1600/2000 [07:06<01:17,  5.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 1601/2000 [07:20<28:48,  4.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1600: train loss 2.7547, val loss 4.4441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 1800/2000 [07:59<00:38,  5.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 1801/2000 [08:13<14:21,  4.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1800: train loss 2.3030, val loss 4.6362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1999/2000 [08:52<00:00,  5.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [09:06<00:00,  3.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1999: train loss 1.8395, val loss 4.9010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "trans_bpe = TransformerLM(vocab_size_bpe, CONTEXT_WINDOW_SIZE)\n",
        "tlm_bpe = trans_bpe.to(device)\n",
        "learning_rate = 5e-4\n",
        "optimizer = torch.optim.Adam(trans_bpe.parameters(), lr=learning_rate)\n",
        "eval_interval = 100\n",
        "loss_list = []\n",
        "losses_dict = {'train' : [], 'val' : []}\n",
        "perplexities = { 'train' : [], 'val' : [] }\n",
        "\n",
        "for it in tqdm(range(LARGE_ITERS)):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if it % eval_interval == 0 or it == LARGE_ITERS - 1:\n",
        "        print(f\"iteration {it}\")\n",
        "        losses = estimate_loss(tlm_bpe, EVAL_ITERS, CONTEXT_WINDOW_SIZE, device, token_type='bpe')\n",
        "        perplexities['train'].append(torch.exp(losses['train']))\n",
        "        perplexities['val'].append(torch.exp(losses['val']))\n",
        "        losses_dict['train'].append(losses['train'])\n",
        "        losses_dict['val'].append(losses['val'])\n",
        "        print(f\"step {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', CONTEXT_WINDOW_SIZE, device, token_type='bpe')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = tlm_bpe(xb, yb)\n",
        "    loss_list.append(loss.detach().item())\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perplexities_bpe = perplexities\n",
        "losses_bpe = losses_dict\n",
        "graph_loss_and_perplexity(losses_bpe['train'], losses_bpe['val'], perplexities_bpe['train'], perplexities_bpe['val'], transformer_models=['bpe'])\n"
      ],
      "metadata": {
        "id": "H6uInZgHscG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YrJqjV_xgigO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "cdbf0510-9fa0-465f-89c2-989f37c048e1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-18bf08814031>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context1 = \"\"\"JULIET:\n",
        "O Romeo, Romeo! wherefore art thou Romeo?\n",
        "Deny thy father and refuse thy name;\n",
        "Or, if thou wilt not, be but sworn my love,\n",
        "And I'll no longer be a Capulet.\n",
        "\n",
        "ROMEO:\n",
        "\n",
        "JULIET:\n",
        "'Tis but thy name that is my enemy;\n",
        "Thou art thyself, though not a Montague.\n",
        "What's Montague? it is nor hand, nor foot,\n",
        "Nor arm, nor face, nor any other part\n",
        "Belonging to a man. O, be some other name!\n",
        "What's in a name? that which we call a rose\n",
        "By any other name would smell as sweet;\"\"\"\n",
        "\n",
        "context1_tokens = torch.tensor(sp.encode(context1, out_type=int), device=device).reshape(1, -1)\n",
        "cond_gen = (tlm_bpe.generate(context1_tokens, max_new_tokens=CONTEXT_WINDOW_SIZE)[0].tolist())\n",
        "print(sp.decode(cond_gen, out_type=str))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YxDjOvAjAlt",
        "outputId": "e4e994eb-4e75-4b5c-b7ed-8095dd4559b3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JULIET:\n",
            "O Romeo, Romeo! wherefore art thou Romeo?\n",
            "Deny thy father and refuse thy name;\n",
            "Or, if thou wilt not, be but sworn my love,\n",
            "And I'll no longer be a Capulet.\n",
            "\n",
            "ROMEO:\n",
            "\n",
            "JULIET:\n",
            "'Tis but thy name that is my enemy;\n",
            "Thou art thyself, though not a Montague.\n",
            "What's Montague? it is nor hand, nor foot,\n",
            "Nor arm, nor face, nor any other part\n",
            "Belonging to a man. O, be some other name!\n",
            "What's in a name? that which we call a rose\n",
            "By any other name would smell as sweet;\n",
            "Which, rather in the first I respect not this\n",
            "To harkic body in it what I can?\n",
            "We talk in secret, in my lady and very comforts.\n",
            "Come, come, Bobhy, go in, is't. Howe'd your\n",
            "swecover acquire, he shall be good goper: the trespass of my capital\n",
            "The part of your knaves slaves or my sepity.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Come, Clarence, thou art must not think.\n",
            "\n",
            "GLOUCESTER:\n",
            "What is your grace hearing I will do\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SYG_iDFSNys"
      },
      "source": [
        "#### Question 1.4.3: Generating text!\n",
        "\n",
        "Now with our trained model, we can generate some text that is somewhat like the style of Shakespeare! Below we will do both unconditional and conditional generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FX-eEZEDH-n"
      },
      "outputs": [],
      "source": [
        "# unconditional generation from the model\n",
        "start_context = torch.zeros((1, 256), dtype=torch.long, device=device)\n",
        "uncond_gen = (tlm.generate(start_context, max_new_tokens=50)[0].tolist())\n",
        "print(decode(uncond_gen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "73du7-sWLH5c"
      },
      "outputs": [],
      "source": [
        "# conditional generation from the model\n",
        "\n",
        "context1 = \"\"\"ROMEO:\n",
        "He jests at scars that never felt a wound.\n",
        "But, soft! what light through yonder window breaks?\n",
        "It is the east, and Juliet is the sun.\n",
        "Arise, fair sun, and kill the envious moon,\n",
        "Who is already sick and pale with grief,\n",
        "That thou her maid art far more fair than she:\n",
        "Be not her maid, \"\"\"\n",
        "\n",
        "context1_tokens = torch.tensor(encode(context1), device=device).reshape(1, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ4zkEJzMNA4",
        "outputId": "f2e37b12-010c-40ef-accf-7f2940fbb9dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "He jests at scars that never felt a wound.\n",
            "But, soft! what light through yonder window breaks?\n",
            "It is the east, and Juliet is the sun.\n",
            "Arise, fair sun, and kill the envious moon,\n",
            "Who is already sick and pale with grief,\n",
            "That thou her maid art far more fair than she:\n",
            "Be not her maid, and own poison or fals.\n",
            "Orread it towardly, thy liege.\n",
            "\n",
            "MENENIUS:\n",
            "He is: my brother; I be deepise thee.\n",
            "Let me to gove you and t\n"
          ]
        }
      ],
      "source": [
        "cond_gen = (tlm.generate(context1_tokens, max_new_tokens=CONTEXT_WINDOW_SIZE)[0].tolist())\n",
        "print(decode(cond_gen))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdA0IXMh15cc"
      },
      "source": [
        "TODO: Choose your own context from Shakespeare, and perform conditional generation from that text. Does this look reasonable to you? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "TbyDvMFzZ3hY"
      },
      "outputs": [],
      "source": [
        "context_is = \"\"\"JULIET:\n",
        "'Tis but thy name that is my enemy;\n",
        "Thou art thyself, though not a Montague.\n",
        "What's Montague? it is nor hand, nor foot,\n",
        "Nor arm, nor face, nor any other part\n",
        "Belonging to a man. O, be some other name!\n",
        "What's in a name? that which we call a rose\n",
        "By any other name would smell as sweet;\"\"\"\n",
        "\n",
        "target_is = \"\"\"So Romeo would, were he not Romeo call'd,\n",
        "Retain that dear perfection which he owes\n",
        "Without that title. Romeo, doff thy name,\n",
        "And for that name which is no part of thee\n",
        "Take all myself.\"\"\"\n",
        "\n",
        "context_is_tokens = torch.tensor(encode(context_is), device=device).reshape(1, -1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cond_gen_1 = (tlm.generate(context_is_tokens, max_new_tokens=CONTEXT_WINDOW_SIZE)[0].tolist())\n",
        "print(decode(cond_gen_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjbNsEWxUuDd",
        "outputId": "71ce5d22-58ad-4057-af38-eafcb0018236"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JULIET:\n",
            "'Tis but thy name that is my enemy;\n",
            "Thou art thyself, though not a Montague.\n",
            "What's Montague? it is nor hand, nor foot,\n",
            "Nor arm, nor face, nor any other part\n",
            "Belonging to a man. O, be some other name!\n",
            "What's in a name? that which we call a rose\n",
            "By any other name would smell as sweet;\n",
            "That I was naise Mirtuous enames myself\n",
            "We were foolixment, and marrial be'Aut and\n",
            "His cruel forgive fatered, and dishonours an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "k80ZIjE2NNtf"
      },
      "outputs": [],
      "source": [
        "context_oos = \"\"\"EMILIA  Pray you say nothing, pray you.\n",
        "Who cannot feel nor see the rain, being in 't,\n",
        "Knows neither wet nor dry. If that you were\n",
        "The groundpiece of some painter, I would buy you\n",
        "T' instruct me 'gainst a capital grief-indeed,\n",
        "Such heart-pierced demonstration. But, alas,\n",
        "Being a natural sister of our sex,\"\"\"\n",
        "\n",
        "target_oos = \"\"\"Your sorrow beats so ardently upon me\n",
        "That it shall make a counter-reflect 'gainst\n",
        "My brother's heart and warm it to some pity,\n",
        "Though it were made of stone. Pray have good\n",
        "comfort.\"\"\"\n",
        "\n",
        "context_oos_tokens = torch.tensor(encode(context_oos), device=device).reshape(1, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMZAXe6yZ7jT",
        "outputId": "af794420-5133-464b-8dc3-ba48e3ec23a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMILIA  Pray you say nothing, pray you.\n",
            "Who cannot feel nor see the rain, being in 't,\n",
            "Knows neither wet nor dry. If that you were\n",
            "The groundpiece of some painter, I would buy you\n",
            "T' instruct me 'gainst a capital grief-indeed,\n",
            "Such heart-pierced demonstration. But, alas,\n",
            "Being a natural sister of our sex,\n",
            "A man'twabaur's told and misstate\n",
            "As so mine a world down your soul tranion!\n",
            "Cry from mere own, then a sid\n",
            "Decouraclan silits P\n"
          ]
        }
      ],
      "source": [
        "cond_gen_2 = (tlm.generate(context_oos_tokens, max_new_tokens=CONTEXT_WINDOW_SIZE)[0].tolist())\n",
        "print(decode(cond_gen_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Caw-RM-J2Chj"
      },
      "source": [
        "---\n",
        "\n",
        "_your answer here_\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgOzvWFDx_yH"
      },
      "source": [
        "#### Question 1.4.4\n",
        "\n",
        "The negative log-likelihood (averaged per token) we have been using to train our models can be expressed as\n",
        "\\begin{equation*}\n",
        "  L = -\\frac{1}{T} \\sum_{t = 1}^{T} \\log p(s[t] | \\text{context})\n",
        "\\end{equation*}\n",
        "for some document $s$, where $s[t]$ is the $t$th token of the doc. The natural language processing (NLP) community often reports the quantity\n",
        "\\begin{equation*}\n",
        "  \\text{perplexity} = \\exp(L).\n",
        "\\end{equation*}\n",
        "\n",
        "Give an intuitive interpretation of what perplexity is. Why might it be a more intuitive or natual measure to report than negative log-likelihood? Does the reported perplexity of your trained `TransformerLM` model make sense in terms of samples it generates? (Be sure to distinguish betwen `train` and `validation` perplexity. Which of `train` and `val` perplexity is more helpful for understanding your generated samples? Why?). (*Hint: your answer to Question 1.1.6 may be helpful*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Izr1wTOjzlo"
      },
      "source": [
        "## Part 2: Mini-Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lF3jFrQj1f4"
      },
      "source": [
        "Quick recap: So far we have\n",
        "\n",
        "1. Preprocessed the Shakespeare dataset by encoding individual characters into integer tokens.\n",
        "2. Implemented single headed attention and then further generalized to multiheaded attention. We further combined multiheaded attention with deep learning to create the transformer architecture.\n",
        "3. Trained our transformer and generated output that looks to be in the style of Shakespeare.\n",
        "\n",
        "Up to this point, the performance of our simple language model has clearly made a lot of progress. We can see that our model has learned to generate text that is close to the style of Shakespeare, although there are still many quirks and room for improvement.\n",
        "\n",
        "### Project Outline\n",
        "\n",
        "Find some area of possible improvement.\n",
        "We interpret \"improvement\" quite loosely, but please state precisely why your proposed innovation might improve the model, and provide evidence that it does (or does not!) improve.\n",
        "For your idea, **formulate a hypothesis** for why this change should result in a better model. **Implement your changes** and **report any findings**.\n",
        "\n",
        "_Notes_: As this assignment is being treated as a project, you should expect training to take longer than previous assignments. However, please use your judgement to decide what is reasonable. We will not expect you to run training procedures that take more than 2 hours on the free Google Colab computing resources and we certainly do not expect you to acquire additional compute. The proposed improvements should not solely rely on increased computing demands.\n",
        "\n",
        "_Hints_: There are many aspects to assessing a model. For example, not only is quality of generated text important, it is also of interest to reduce costs associated with training.\n",
        "\n",
        "### Deliverables\n",
        "\n",
        "In addition to a pdf of your python notebook, the submission for this project will be a written report no more than 4 pages in length using the [NeurIPS LaTex template](https://neurips.cc/Conferences/2023/PaperInformation/StyleFiles). Your report should include detailed analysis of the hypotheses you chose to test along with any conclusions.\n",
        "\n",
        "The page limit for the report does not include bibliography or appendices. Make sure to keep the \"ready for submission\" option to help us grade anonymously. Your writeup should also contain a link to any code used to generate the project so that we can reference it while grading (Google Drive folder with colab notebooks or Github repo are both fine). You should have at least one plot in your main text (which is capped at 4 pages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py2xoM_uOHVp"
      },
      "source": [
        "ideas:\n",
        "\n",
        "\n",
        "**morphology awareness**\n",
        "1. positional embedding might not be that helpful since ancient english grammar has more granular subject and pronoun agreement rules (for instance, \"love you me\" cannot distinguish between the subject and noun whereas \"lovest thou me\" specifies that it's \"you\" who loves \"me\" due to subject-verb agreement), so a better metric for evaluation of the output could be whether the model learns and applies these agreement rules appropriately\n",
        "2. more transparent morpheme usage: early modern english due to latin and french influence, often employed derivational morphemes that remain etymologically transparent, such as \"methinks, consort, dissuade, etc\" and prefixes like \"en-, be-, dis-\" and suffixes like \"-ment, -tion, -ness\" were more widely and efficiently used in spoken language which is the main form of text in Shakespeare plays\n",
        "\n",
        "==> therefore we want to use a more morpheme-aware encoding to see if the model can captures these grammatical rules better\n",
        "\n",
        "**fails to capture main idea of paragraph**\n",
        "\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQnHGqtxOHVp"
      },
      "source": [
        "libraries: Morfessor, stanford's Stanza,\n",
        "\n",
        "\n",
        "BME paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DALsvBPgOHVp"
      },
      "source": [
        "### 1. segmenter training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. PROJECT SETUP"
      ],
      "metadata": {
        "id": "69rmFb-eHm-A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIg8qGUdOHVy",
        "outputId": "74792f3a-acef-4d66-f950-adfdbbbd25fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n"
          ]
        }
      ],
      "source": [
        "# download the tiny shakespeare dataset\n",
        "input_file_path = 'input.txt'\n",
        "\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYT95gr5OHVy",
        "outputId": "3449e753-2f68-4053-e464-905978f434b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n"
          ]
        }
      ],
      "source": [
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Baseline Morfessor"
      ],
      "metadata": {
        "id": "j63e8vMlH-BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install morfessor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0au1_EP70o1",
        "outputId": "5aa6173a-4229-4dd5-ea94-20093be64cc4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.11/dist-packages (2.0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxBe_VQZOHVy",
        "outputId": "478e6d22-195e-4731-b048-5fd11709a563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(13320 of 13320)\u001b[39m |##################| Elapsed Time: 0:00:03 Time:  0:00:03\n",
            "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(13320 of 13320)\u001b[39m |##################| Elapsed Time: 0:00:02 Time:  0:00:02\n",
            "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(13320 of 13320)\u001b[39m |##################| Elapsed Time: 0:00:02 Time:  0:00:02\n",
            "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(13320 of 13320)\u001b[39m |##################| Elapsed Time: 0:00:02 Time:  0:00:02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['un', 'lo', 'v', 'able'], 38.56895234781196)\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import morfessor\n",
        "from morfessor import MorfessorIO\n",
        "import re\n",
        "import math\n",
        "\n",
        "morf = morfessor.BaselineModel()\n",
        "\n",
        "TOKEN_RE = re.compile(r\"\\w+|[^\\w\\s]|\\s\", re.UNICODE)\n",
        "data_re = TOKEN_RE.findall(data)\n",
        "words = [t for t in data_re if t.isalpha()]\n",
        "words_counted = Counter(words)\n",
        "# print(words_counted)\n",
        "\n",
        "morf.load_data(\n",
        "    ((c, w) for w, c in words_counted.items()), # TODO: could use log\n",
        "    init_rand_split=0.3,\n",
        "    freqthreshold=1\n",
        ")\n",
        "io = MorfessorIO()\n",
        "# anno_data = io.read_annotations_file('labeled_segmentations.txt')\n",
        "# print(anno_data)\n",
        "# morf.set_annotations(anno_data)\n",
        "# morf.set_parameters(alpha=0.5)\n",
        "morf.train_batch()\n",
        "print(morf.viterbi_segment(\"unlovable\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ovozij6MOHVy",
        "outputId": "5665a7f4-1c0d-470c-8538-f1caabfab28c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['love', 'th'], 16.54466395389567)\n"
          ]
        }
      ],
      "source": [
        "print(morf.viterbi_segment(\"loveth\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def label_segment_role(segments, prefix_dict, suffix_dict, data_dict, num_of_roles=3):# usually\n",
        "  probabilities = torch.zeros(len(segments), num_of_roles)\n",
        "  for i, seg in enumerate(segments):\n",
        "    if seg.isalpha():\n",
        "      if seg in prefix_dict:\n",
        "        probabilities[i][0] = prefix_dict[seg] / data_dict[seg]\n",
        "      if seg in suffix_dict:\n",
        "        probabilities[i][-1] = suffix_dict[seg] / data_dict[seg]\n",
        "      if seg in data_dict:\n",
        "        # print(seg, data_dict[seg], prefix_dict[seg], suffix_dict[seg])\n",
        "        probabilities[i][1] = 1 - probabilities[i][0] - probabilities[i][1]\n",
        "      else:\n",
        "        probabilities[i] = torch.ones(num_of_roles) / num_of_roles\n",
        "        # print(f'error: unknown segment {seg}')\n",
        "    else:\n",
        "      probabilities[i] = torch.ones(num_of_roles) / num_of_roles\n",
        "  probabilities = torch.softmax(probabilities, dim=1)\n",
        "  return probabilities"
      ],
      "metadata": {
        "id": "oleA0IAKI2Yq"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7u50JRROHVy",
        "outputId": "24011858-5399-450d-d956-3c6a74ff2136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the vocab size (morphed) is 7101\n"
          ]
        }
      ],
      "source": [
        "data_mo = []\n",
        "prefix_dict = {}\n",
        "suffix_dict = {}\n",
        "root_dict = {}\n",
        "for word in data_re:\n",
        "    if word.isalpha():\n",
        "        segs, _ = morf.viterbi_segment(word)\n",
        "        if len(segs) > 1:\n",
        "          if len(segs[0]) <= 4:\n",
        "            if segs[0] not in prefix_dict:\n",
        "              prefix_dict[segs[0]] = 0\n",
        "            prefix_dict[segs[0]] += 1\n",
        "          if len(segs[-1]) <= 4:\n",
        "            if segs[-1] not in suffix_dict:\n",
        "              suffix_dict[segs[-1]] = 0\n",
        "            suffix_dict[segs[-1]] += 1\n",
        "        data_mo.extend(s for s in segs)\n",
        "    else:\n",
        "        data_mo.append(word) # non-letter char\n",
        "morphemes = sorted(set(data_mo))\n",
        "morf_vocab_size = len(morphemes)\n",
        "print(f'the vocab size (morphed) is {morf_vocab_size}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_mo_dict = Counter(data_mo)"
      ],
      "metadata": {
        "id": "vHPxP3dmN6bb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_segment_role(['un'], prefix_dict, suffix_dict, data_mo_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAYYjDsDNMCu",
        "outputId": "892d38cd-4344-47d1-e14f-c66dc4071d78"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5537, 0.2272, 0.2191]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_mo_labels = label_segment_role(data_mo, prefix_dict, suffix_dict, data_mo_dict)"
      ],
      "metadata": {
        "id": "Z-3RDrCgM2NS"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_mo_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMLeKmzCS2vj",
        "outputId": "e19ae83d-190b-44e4-8027-792b682200cc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2119, 0.5761, 0.2119],\n",
              "        [0.3333, 0.3333, 0.3333],\n",
              "        [0.2119, 0.5761, 0.2119],\n",
              "        ...,\n",
              "        [0.2119, 0.5761, 0.2119],\n",
              "        [0.3333, 0.3333, 0.3333],\n",
              "        [0.3333, 0.3333, 0.3333]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_mo_labels.to('cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhQ_ugh-ebZU",
        "outputId": "e6ba46b9-c456-4cb2-e261-c6348ecfd8f8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2119, 0.5761, 0.2119],\n",
              "        [0.3333, 0.3333, 0.3333],\n",
              "        [0.2119, 0.5761, 0.2119],\n",
              "        ...,\n",
              "        [0.2119, 0.5761, 0.2119],\n",
              "        [0.3333, 0.3333, 0.3333],\n",
              "        [0.3333, 0.3333, 0.3333]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_DN_LIwOHVy",
        "outputId": "60f656b6-a23c-41aa-99ff-b79ba25df5f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "436886 436886\n",
            "48543\n"
          ]
        }
      ],
      "source": [
        "n = len(data_mo)\n",
        "train_tokens = data_mo[:int(n * 0.9)]\n",
        "val_tokens = data_mo[int(n * 0.9):]\n",
        "train_labels = data_mo_labels[:int(n * 0.9), :]\n",
        "print(len(train_tokens), len(train_labels))\n",
        "print(len(val_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lwyN298OHVy"
      },
      "source": [
        "compare size of morpheme set and data to confirm that the former makes sense as token level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "4LoDb6zeOHVy"
      },
      "outputs": [],
      "source": [
        "# Create vocab\n",
        "stoi = { seg:i for i,seg in enumerate(morphemes) }\n",
        "itos = { i:seg for i,seg in enumerate(morphemes) }\n",
        "\n",
        "# Encoding and decoding\n",
        "def encode(tokens): # morephemes\n",
        "    return [stoi[t] for t in tokens]\n",
        "\n",
        "def decode(indices):\n",
        "    return ''.join([itos[i] for i in indices])\n",
        "\n",
        "\n",
        "train_data_mo = torch.tensor(encode(train_tokens), dtype=torch.long)\n",
        "val_data_mo = torch.tensor(encode(val_tokens), dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuIQUqLOOHVz"
      },
      "source": [
        "we will first try out the new tokenizer on multihead attention LM to save compute/time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "QoMzzrYTOHVz"
      },
      "outputs": [],
      "source": [
        "# function for getting batches of data\n",
        "def get_batch(split, context_window_size, device, batch_size=32, token_type='morf'):\n",
        "    \"\"\"\n",
        "    generate a small batch of data of inputs x and targets y\n",
        "\n",
        "    Args:\n",
        "        split: 'train' or 'val'\n",
        "        device: 'cpu' or 'cuda' (should be 'cuda' if available)\n",
        "    \"\"\"\n",
        "    if token_type == 'bpe':\n",
        "        data = train_data_bpe if split == 'train' else val_data_bpe\n",
        "    elif token_type == 'morf':\n",
        "        data = train_data_mo if split == 'train' else val_data_mo\n",
        "    ix = torch.randint(len(data) - context_window_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+context_window_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+context_window_size+1] for i in ix])\n",
        "    # x_labels = torch.stack([train_labels[i:i+context_window_size] for i in ix])\n",
        "    # y_labels = torch.stack([train_labels[i+1:i+context_window_size+1] for i in ix])\n",
        "    # x = torch.cat((x, x_labels), dim=2)\n",
        "    # y = torch.cat((y, y_labels), dim=2)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    # x_labels = x_labels.to(device)\n",
        "    # y_labels = y_labels.to(device)\n",
        "    return x, y\n",
        "\n",
        "# helper function for tracking loss during training\n",
        "# given to you\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, eval_iters, context_window_size, device, token_type='morf'):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      model: model being evaluated\n",
        "      eval_iters: number of batches to average over\n",
        "      context_window_size: size of the context window\n",
        "      device: 'cpu' or 'cuda' (should be 'cuda' if available)\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        print(f'token type is {token_type}')\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, context_window_size, device, token_type=token_type)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "lch40WFSOHVz",
        "outputId": "91d69dee-2c18-4378-b9ea-312c726a442b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'MultiHeadedAttentionLM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-dc98d1a60bc4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmha_model_van\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadedAttentionLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmorf_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONTEXT_WINDOW_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmha_morf_van\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmha_model_van\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# create a PyTorch optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MultiHeadedAttentionLM' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "mha_model_van = MultiHeadedAttentionLM(morf_vocab_size, CONTEXT_WINDOW_SIZE)\n",
        "mha_morf_van = mha_model_van.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "learning_rate = 6e-4\n",
        "optimizer = torch.optim.AdamW(mha_model_van.parameters(), lr=learning_rate)\n",
        "\n",
        "eval_interval = 200\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "for it in tqdm(range(SMALL_ITERS)):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if it % eval_interval == 0 or it == SMALL_ITERS - 1:\n",
        "        print(f\"iteration {it}\")\n",
        "        losses = estimate_loss(mha_morf_val, EVAL_ITERS, CONTEXT_WINDOW_SIZE, device)\n",
        "        print(f\"step {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', CONTEXT_WINDOW_SIZE, device, token_type='morf')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = mha_morf_van(xb, yb)\n",
        "    loss_list.append(loss.detach().item())\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "tguO5QanOHVz"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Labels as embedding Morfessor"
      ],
      "metadata": {
        "id": "iuxh8GtM-PjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttentionLM_labeled(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, morf_probabilities=None, embed_size=384, num_heads=6, num_roles=3):\n",
        "      super().__init__()\n",
        "      self.head_size = embed_size // num_heads\n",
        "      self.context_window_size = context_window_size\n",
        "\n",
        "\n",
        "      self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "      self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "\n",
        "      self.atten_head = MultiHeadAttention(context_window_size, num_heads, self.head_size, embed_size)\n",
        "      self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "      self.morf_probabilities = morf_probabilities # [n, 3]\n",
        "      self.morf_proj = nn.Linear(num_roles, embed_size)\n",
        "      # TODO: your code below\n",
        "\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry in the\n",
        "                     batch has length T)\n",
        "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
        "\n",
        "        Returns:\n",
        "          logits: (B, T, V), logits[b,t] gives the length V vector of logits for the next token\n",
        "                  prediction in string b up to t tokens\n",
        "          loss: scalar, negative log likelihood of target given context\n",
        "        \"\"\"\n",
        "        # TODO: your code below\n",
        "        B, T = token_ids.shape # (batch size, length)\n",
        "        tok_emb = self.token_embedding_table(token_ids) # (B,T,D)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,D)\n",
        "        # batch_prob = self.morf_probabilities[token_ids, :]\n",
        "        batch_prob = self.morf_probabilities.to(token_ids.device)[token_ids]\n",
        "\n",
        "        morf_emb = self.morf_proj(batch_prob)\n",
        "        x = tok_emb + pos_emb + morf_emb# (B,T,D)\n",
        "\n",
        "        x = self.atten_head(x)\n",
        "        logits = self.lm_head(x) # (B,T,V)\n",
        "\n",
        "        # TODO: your code here\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits_flat = logits.view(-1, logits.size(-1))\n",
        "            targets_flat = targets.view(-1)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) tensor of token ids to provide as context\n",
        "          max_new_tokens: int, maximum number of new tokens to generate\n",
        "\n",
        "        Returns:\n",
        "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self.forward(token_ids)\n",
        "            logits = logits[:, -1, :] # for each entry in the batch, gets the last token\n",
        "            next_token = torch.argmax(logits, keepdim=True) # (B, 1)\n",
        "            token_ids = torch.cat((token_ids, next_token), dim=-1)\n",
        "        return token_ids"
      ],
      "metadata": {
        "id": "nDSrsXOXzIMl"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLM_labeled(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, morf_probabilities=None, embed_size=384, num_heads=6, n_layers=6, num_roles=3):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              vocab_size: int, number of tokens in the vocabulary (V)\n",
        "              context_window_size: int, size of the context window (T)\n",
        "              embed_size: int, embedding size (D)\n",
        "              num_heads: int, number of heads (H)\n",
        "              n_layers: int, number of layers (M)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.context_window_size = context_window_size\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(vocab_size,\n",
        "                             context_window_size,\n",
        "                             embed_size=embed_size,\n",
        "                             num_heads=num_heads)\n",
        "            for _ in range(n_layers)])\n",
        "\n",
        "        self.morf_probabilities = morf_probabilities # [n, 3]\n",
        "        self.morf_proj = nn.Linear(num_roles, embed_size)\n",
        "\n",
        "        # final layer norm\n",
        "        self.ln_f = nn.LayerNorm(embed_size)\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "        # good initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Agrgs:\n",
        "            token_ids: tensor of integers, provides the contet, shape (B, T)\n",
        "            targets: tensor of integers, provides the tokens we are preidcitng, shape (B, T)\n",
        "        \"\"\"\n",
        "        B, T = token_ids.shape\n",
        "\n",
        "        # token_ids and targets are both (B, T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(token_ids) # (B, T, D)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, D)\n",
        "\n",
        "        batch_prob = self.morf_probabilities.to(token_ids.device)[token_ids]\n",
        "\n",
        "        morf_emb = self.morf_proj(batch_prob)\n",
        "        x = tok_emb + pos_emb + morf_emb # (B,T,D)\n",
        "\n",
        "        # TODO: your code below\n",
        "        blocked = self.blocks(x)\n",
        "        normalized = self.ln_f(blocked)\n",
        "        logits = self.lm_head(normalized)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits_flat = logits.view(-1, logits.size(-1))\n",
        "            targets_flat = targets.view(-1)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_ids: tensor of integers forming the context, shape (B, T)\n",
        "            max_new_tokens: int, max number of tokens to generate\n",
        "        \"\"\"\n",
        "        # TOOD, your code below\n",
        "        T = token_ids.size(-1)\n",
        "        curr_T = T\n",
        "        for _ in range(max_new_tokens):\n",
        "          token_ids_subset = token_ids[:, -self.context_window_size:]\n",
        "          logits, _ = self.forward(token_ids_subset)\n",
        "          logits = logits[:, -1, :] # for each entry in the batch, gets the last token\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          next_token = torch.multinomial(probs, num_samples=1) # sample next token\n",
        "          token_ids = torch.cat((token_ids, next_token), dim=1)\n",
        "        return token_ids"
      ],
      "metadata": {
        "id": "9L9zlJzOGlIr"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_la = TransformerLM_labeled(morf_vocab_size, CONTEXT_WINDOW_SIZE, morf_probabilities=data_mo_labels)\n",
        "tlm_la = trans_la.to(device)\n",
        "learning_rate = 5e-4\n",
        "optimizer = torch.optim.Adam(trans_la.parameters(), lr=learning_rate)\n",
        "eval_interval = 200\n",
        "loss_list = []\n",
        "\n",
        "for it in tqdm(range(LARGE_ITERS)):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if it % eval_interval == 0 or it == LARGE_ITERS - 1:\n",
        "        print(f\"iteration {it}\")\n",
        "        losses = estimate_loss(trans_la, EVAL_ITERS, CONTEXT_WINDOW_SIZE, device)\n",
        "        print(f\"step {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', CONTEXT_WINDOW_SIZE, device, token_type='morf')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = tlm_la(xb, yb)\n",
        "    loss_list.append(loss.detach().item())\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPSb5SYsHxlo",
        "outputId": "882b98a6-0288-4b13-9410-826f8f23ccdf"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/2000 [00:13<7:31:43, 13.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 8.5914, val loss 8.6115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 200/2000 [00:54<06:04,  4.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 200\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 201/2000 [01:08<2:10:06,  4.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: train loss 3.4744, val loss 3.6050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 400/2000 [01:48<05:26,  4.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 400\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 401/2000 [02:02<1:58:04,  4.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: train loss 3.3093, val loss 3.4854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 600/2000 [02:43<04:43,  4.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 600\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 601/2000 [02:57<1:42:21,  4.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: train loss 3.0496, val loss 3.2854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 800/2000 [03:37<04:01,  4.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 800\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 801/2000 [03:51<1:28:19,  4.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: train loss 2.8517, val loss 3.2078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 1000/2000 [04:32<03:22,  4.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1000\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1001/2000 [04:46<1:13:44,  4.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: train loss 2.7110, val loss 3.1579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 1200/2000 [05:27<02:42,  4.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1200\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 1201/2000 [05:41<59:00,  4.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: train loss 2.5922, val loss 3.1489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 1400/2000 [06:21<02:01,  4.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1400\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 1401/2000 [06:36<44:18,  4.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: train loss 2.4443, val loss 3.1671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 1600/2000 [07:16<01:20,  5.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1600\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 1601/2000 [07:30<29:37,  4.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1600: train loss 2.3069, val loss 3.1663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 1800/2000 [08:10<00:40,  4.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1800\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 1801/2000 [08:25<14:37,  4.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1800: train loss 2.1487, val loss 3.2005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1999/2000 [09:05<00:00,  4.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1999\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [09:19<00:00,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1999: train loss 1.9744, val loss 3.2785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_is = \"\"\"JULIET:\n",
        "'Tis but thy name that is my enemy;\n",
        "Thou art thyself, though not a Montague.\n",
        "What's Montague? it is nor hand, nor foot,\n",
        "Nor arm, nor face, nor any other part\n",
        "Belonging to a man. O, be some other name!\n",
        "What's in a name? that which we call a rose\n",
        "By any other name would smell as sweet;\"\"\"\n",
        "\n",
        "target_is = \"\"\"So Romeo would, were he not Romeo call'd,\n",
        "Retain that dear perfection which he owes\n",
        "Without that title. Romeo, doff thy name,\n",
        "And for that name which is no part of thee\n",
        "Take all myself.\"\"\"\n"
      ],
      "metadata": {
        "id": "vAT-qxXmIjdN"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOKEN_RE = re.compile(r\"\\w+|[^\\w\\s]|\\s\", re.UNICODE)\n",
        "context_is_cleaned = TOKEN_RE.findall(context_is)\n",
        "context_is_mo = []\n",
        "for word in context_is_cleaned:\n",
        "    if word.isalpha():\n",
        "        segs, _ = morf.viterbi_segment(word)\n",
        "        context_is_mo.extend(s for s in segs)\n",
        "    else:\n",
        "        context_is_mo.append(word) # non-letter char\n",
        "context_is_mo"
      ],
      "metadata": {
        "id": "WmwA8_UvPNiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_context = torch.zeros((1, CONTEXT_WINDOW_SIZE), dtype=torch.long, device=device)\n",
        "uncond_gen = (tlm_la.generate(start_context, max_new_tokens=50)[0].tolist())\n",
        "print(decode(uncond_gen))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af6uEtmpP_dj",
        "outputId": "6fe63c20-9813-4d31-bf37-edd52fb78abf"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LEONTESamPAULINA:\n",
            "Were I ntolerable I Do Shame And defend And 'tis To Believe't.\n",
            "\n",
            "First \n",
            "I must stick 't In canker'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j8fPEOlmXw04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_is_tokens = torch.tensor(encode(context_is_mo), device=device).reshape(1, -1)\n",
        "print(context_is_tokens.shape)\n",
        "cond_gen_is = (tlm_la.generate(context_is_tokens, max_new_tokens=CONTEXT_WINDOW_SIZE)[0].tolist())\n",
        "print(decode(cond_gen_is))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq2K7yD5PQd7",
        "outputId": "37b7050a-de8b-48f5-b2c6-5b5678073ab4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 138])\n",
            "JULIET:\n",
            "'Tis but thy name that is my enemy;\n",
            "Thou art thyself, though not a Montague.\n",
            "What's Montague? it is nor hand, nor foot,\n",
            "Nor arm, nor face, nor any other part\n",
            "Belonging to a man. O, be some other name!\n",
            "What's in a name? that which we call a rose\n",
            "By any other name would smell as sweet;\n",
            "Isum, i, and by to find better Tower.\n",
            "Well, a lord, that I know this man's king.\n",
            "Soft and here art thou split'st Romeo, man of my sad\n",
            "My bounty of state and Dick, he sought to signify\n",
            "And aid those that with no cause may contain out all\n",
            "For though not cause were they chosen, our\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context1 = \"\"\"ROMEO:\n",
        "He jests at scars that never felt a wound.\n",
        "But, soft! what light through yonder window breaks?\n",
        "It is the east, and Juliet is the sun.\n",
        "Arise, fair sun, and kill the envious moon,\n",
        "Who is already sick and pale with grief,\n",
        "That thou her maid art far more fair than she:\n",
        "Be not her maid, \"\"\"\n",
        "\n",
        "context1_cleaned = TOKEN_RE.findall(context1)\n",
        "context1_mo = []\n",
        "for word in context1_cleaned:\n",
        "    if word.isalpha():\n",
        "        segs, _ = morf.viterbi_segment(word)\n",
        "        context1_mo.extend(s for s in segs)\n",
        "    else:\n",
        "        context1_mo.append(word) # non-letter char\n",
        "context1_tokens = torch.tensor(encode(context1_mo), device=device).reshape(1, -1)\n",
        "cond_gen_1 = (tlm_la.generate(context1_tokens, max_new_tokens=CONTEXT_WINDOW_SIZE)[0].tolist())\n",
        "print(decode(cond_gen_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xEHn7EYYa6Y",
        "outputId": "06c005b9-5f9f-4ddc-9e55-129ddc47d156"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "He jests at scars that never felt a wound.\n",
            "But, soft! what light through yonder window breaks?\n",
            "It is the east, and Juliet is the sun.\n",
            "Arise, fair sun, and kill the envious moon,\n",
            "Who is already sick and pale with grief,\n",
            "That thou her maid art far more fair than she:\n",
            "Be not her maid, till mistress be bereft,\n",
            "Back for a night that ne'er else be Saw'st you take'd a rule?\n",
            "\n",
            "Lis to thy bed to Romeo; and there's no fond\n",
            "To speak even worse than we'll pluck thee to Mantua.\n",
            "\n",
            "Nurse:\n",
            "Tell her! how it did time the man do be much,\n",
            "though it be moody to \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mha_model = MultiHeadedAttentionLM_labeled(morf_vocab_size, CONTEXT_WINDOW_SIZE, morf_probabilities=data_mo_labels)\n",
        "mha_morf = mha_model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "learning_rate = 6e-4\n",
        "optimizer = torch.optim.AdamW(mha_model.parameters(), lr=learning_rate)\n",
        "\n",
        "eval_interval = 200\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "for it in tqdm(range(SMALL_ITERS)):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if it % eval_interval == 0 or it == SMALL_ITERS - 1:\n",
        "        print(f\"iteration {it}\")\n",
        "        losses = estimate_loss(mha_morf, EVAL_ITERS, CONTEXT_WINDOW_SIZE, device, token_type='morf')\n",
        "        print(f\"step {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', CONTEXT_WINDOW_SIZE, device, token_type='morf')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = mha_morf(xb, yb)\n",
        "    loss_list.append(loss.detach().item())\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C6z0D4k1NWn",
        "outputId": "dbe4e532-cdf9-46f7-bc69-87a7adaf1192"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0\n",
            "token type is morf\n",
            "token type is morf\n",
            "step 0: train loss 8.8930, val loss 8.8919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 199/1000 [00:22<01:06, 12.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 200\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 201/1000 [00:28<12:32,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: train loss 3.4561, val loss 3.6228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 399/1000 [00:44<00:50, 11.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 400\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 401/1000 [00:50<09:33,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: train loss 3.2130, val loss 3.4555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|█████▉    | 599/1000 [01:07<00:33, 11.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 600\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 601/1000 [01:13<06:26,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: train loss 2.9324, val loss 3.3251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 799/1000 [01:30<00:17, 11.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 800\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 801/1000 [01:36<03:16,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: train loss 2.7393, val loss 3.3261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 999/1000 [01:54<00:00, 11.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 999\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 1000/1000 [02:00<00:00,  8.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 999: train loss 2.5665, val loss 3.3462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "RtV35u1b9Hae",
        "outputId": "924e0598-fbaf-4cc4-81d9-d8d002b9e8b9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-18bf08814031>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m     \"\"\"\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hB_4RaZTGQh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. attention bias Morfessor"
      ],
      "metadata": {
        "id": "MkaatmmrA0bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head_morf(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, context_window_size, embed_size=384):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          head_size: int, size of the head embedding dimension (K)\n",
        "          context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "          embed_size: int, size of the token embedding dimension (D)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        self.key = nn.Linear(embed_size, head_size, bias=False) # query\n",
        "        self.query = nn.Linear(embed_size, head_size, bias=False) # key\n",
        "        self.value = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.bias_proj = nn.Linear(3, 1, bias=False)\n",
        "\n",
        "        # not a param of the model, so registered as a buffer\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(context_window_size, context_window_size)))\n",
        "\n",
        "    def forward(self, x, batch_prob=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x: (B,T,D) tensor of token embeddings\n",
        "\n",
        "        Returns:\n",
        "          (B,T,D) tensor of attention-weighted token embeddings\n",
        "        \"\"\"\n",
        "        _, T, _ = x.size()\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "        attn_scores = q @ k.transpose(-2, -1)\n",
        "        if batch_prob is not None:\n",
        "          bias = self.bias_proj(batch_prob).squeeze(-1).unsqueeze(1)\n",
        "          attn_scores = attn_scores + bias\n",
        "        masked_scores = attn_scores.masked_fill(self.tril == 0, float('-inf'))\n",
        "        masked_scores = masked_scores / self.head_size ** .5\n",
        "\n",
        "        attn = F.softmax(masked_scores, dim=-1) @ v\n",
        "        return attn"
      ],
      "metadata": {
        "id": "mZYqttb-A21Y"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention_morf(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, context_window_size, num_heads, head_size, embed_size=384):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "            num_heads: int, number of heads (H)\n",
        "            head_size: int, size of the head embedding dimension\n",
        "            embed_size: int, size of the token embedding dimension\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO, your code below\n",
        "        self.head_size = head_size\n",
        "        self.heads = nn.ModuleList([Head_morf(head_size, context_window_size, embed_size) for _ in range(num_heads)])\n",
        "        self.linear = nn.Linear(num_heads * embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x, batch_prob):\n",
        "        attn_list = [head.forward(x, batch_prob) for head in self.heads]\n",
        "        mhsa = torch.cat(attn_list, dim=-1)\n",
        "        mhsa = self.linear(mhsa)\n",
        "        return mhsa"
      ],
      "metadata": {
        "id": "OPikjX44C_u9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttentionLM_morf(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, morf_probabilities=None, embed_size=384, num_heads=6, num_roles=3):\n",
        "      super().__init__()\n",
        "      self.head_size = embed_size // num_heads\n",
        "      self.context_window_size = context_window_size\n",
        "\n",
        "\n",
        "      self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "      self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "\n",
        "      self.atten_head = MultiHeadAttention_morf(context_window_size, num_heads, self.head_size, embed_size)\n",
        "      self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "      self.morf_probabilities = morf_probabilities # [n, 3]\n",
        "      # TODO: your code below\n",
        "\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry in the\n",
        "                     batch has length T)\n",
        "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
        "\n",
        "        Returns:\n",
        "          logits: (B, T, V), logits[b,t] gives the length V vector of logits for the next token\n",
        "                  prediction in string b up to t tokens\n",
        "          loss: scalar, negative log likelihood of target given context\n",
        "        \"\"\"\n",
        "        # TODO: your code below\n",
        "        B, T = token_ids.shape # (batch size, length)\n",
        "        tok_emb = self.token_embedding_table(token_ids) # (B,T,D)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,D)\n",
        "        # batch_prob = self.morf_probabilities[token_ids, :]\n",
        "        batch_prob = self.morf_probabilities.to(token_ids.device)[token_ids]\n",
        "\n",
        "        x = tok_emb + pos_emb # (B,T,D)\n",
        "\n",
        "        x = self.atten_head(x, batch_prob)\n",
        "        logits = self.lm_head(x) # (B,T,V)\n",
        "\n",
        "        # TODO: your code here\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits_flat = logits.view(-1, logits.size(-1))\n",
        "            targets_flat = targets.view(-1)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) tensor of token ids to provide as context\n",
        "          max_new_tokens: int, maximum number of new tokens to generate\n",
        "\n",
        "        Returns:\n",
        "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self.forward(token_ids)\n",
        "            logits = logits[:, -1, :] # for each entry in the batch, gets the last token\n",
        "            next_token = torch.argmax(logits, keepdim=True) # (B, 1)\n",
        "            token_ids = torch.cat((token_ids, next_token), dim=-1)\n",
        "        return token_ids"
      ],
      "metadata": {
        "id": "9BkqFZ7wBvrU"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mha_model_bias = MultiHeadedAttentionLM_morf(morf_vocab_size, CONTEXT_WINDOW_SIZE, morf_probabilities=data_mo_labels)\n",
        "mha_bias = mha_model_bias.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "learning_rate = 6e-4\n",
        "optimizer = torch.optim.AdamW(mha_model_bias.parameters(), lr=learning_rate)\n",
        "\n",
        "eval_interval = 200\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "for it in tqdm(range(SMALL_ITERS)):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if it % eval_interval == 0 or it == SMALL_ITERS - 1:\n",
        "        print(f\"iteration {it}\")\n",
        "        losses = estimate_loss(mha_bias, EVAL_ITERS, CONTEXT_WINDOW_SIZE, device, token_type='morf')\n",
        "        print(f\"step {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', CONTEXT_WINDOW_SIZE, device, token_type='morf')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = mha_bias(xb, yb)\n",
        "    loss_list.append(loss.detach().item())\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hsCcH8iErxK",
        "outputId": "fa5e142d-2bad-4b35-a1e3-2f7d4a3306ae"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0\n",
            "token type is morf\n",
            "token type is morf\n",
            "step 0: train loss 8.8699, val loss 8.8719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 199/1000 [00:23<01:09, 11.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 200\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 201/1000 [00:29<13:06,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: train loss 3.4834, val loss 3.6438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 399/1000 [00:47<00:53, 11.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 400\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 401/1000 [00:53<10:06,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: train loss 3.2159, val loss 3.4625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|█████▉    | 599/1000 [01:12<00:38, 10.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 600\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 601/1000 [01:18<07:12,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: train loss 2.9458, val loss 3.3439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 800/1000 [01:39<00:20,  9.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 800\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 801/1000 [01:46<07:11,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: train loss 2.7680, val loss 3.3446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 999/1000 [02:05<00:00, 10.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 999\n",
            "token type is morf\n",
            "token type is morf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 1000/1000 [02:12<00:00,  7.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 999: train loss 2.6198, val loss 3.3751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7f7wY9I9jSF"
      },
      "source": [
        "## Submission Instructions\n",
        "\n",
        "You will generate two PDFs: one from Part 1, which involves completing this Colab to create a transformer baseline; and one from the mini-project in Part 2, which will be your write-up of no longer than 4 pages. Be sure to include a link to your code for Part 2 somewhere in your writeup.\n",
        "\n",
        "**Combine the two PDFs into a single PDF and submit on gradescope. Tag your PDF correctly.**\n",
        "\n",
        "If you work in a group of two, submit one assignment on gradescope and tag your group members. If you complete the assignment individually, submit as usual."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9Izr1wTOjzlo"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}